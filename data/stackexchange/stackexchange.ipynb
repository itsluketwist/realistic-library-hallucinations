{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7403d950",
   "metadata": {},
   "source": [
    "# Download StackExchange data\n",
    "\n",
    "Query the Software Recommendations StackExchange API for questions.\n",
    "\n",
    "Want questions that show the terminology developers use when searching for libraries.\n",
    "\n",
    "Get all questions tagged with 'library', or that have 'library' in the title.\n",
    "\n",
    "Website: https://softwarerecs.stackexchange.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f292bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the client\n",
    "\n",
    "from stackapi import StackAPI\n",
    "\n",
    "site = StackAPI(\"softwarerecs\", max_pages=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc8f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter string queried from https://api.stackexchange.com/docs/create-filter\n",
    "# includes: question_id, title, tags, creation_date, link\n",
    "\n",
    "filter = \"!.yIVcje83OXko3o5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38318ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter the response\n",
    "\n",
    "KEYS_TO_SAVE = [\"title\", \"tags\", \"creation_date\", \"link\"]\n",
    "\n",
    "\n",
    "def filter_response(\n",
    "    response: dict,\n",
    ") -> dict:\n",
    "    return {\n",
    "        item[\"question_id\"]: {key: item[key] for key in KEYS_TO_SAVE if key in item}\n",
    "        for item in response[\"items\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6606a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count all questions with 'library' tag\n",
    "\n",
    "library_tag_total = site.fetch(\n",
    "    endpoint=\"questions\",\n",
    "    tagged=\"library\",\n",
    "    filter=\"total\",\n",
    ")\n",
    "print(\"Response:\", library_tag_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query all questions with 'library' tag\n",
    "\n",
    "library_tag_resp = site.fetch(\n",
    "    endpoint=\"questions\",\n",
    "    tagged=\"library\",\n",
    ")\n",
    "print(f\"Have {len(library_tag_resp['items'])} records.\")\n",
    "print(library_tag_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7066c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count all questions with 'library' in the title\n",
    "\n",
    "library_title_total = site.fetch(\n",
    "    endpoint=\"search/advanced\",\n",
    "    title=\"library\",\n",
    "    filter=\"total\",\n",
    ")\n",
    "print(\"Response:\", library_title_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query all questions with 'library' in the title\n",
    "\n",
    "library_title_resp = site.fetch(\n",
    "    endpoint=\"search/advanced\",\n",
    "    title=\"library\",\n",
    ")\n",
    "print(f\"Have {len(library_title_resp['items'])} records.\")\n",
    "print(library_title_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89899cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the questions\n",
    "\n",
    "questions = {\n",
    "    **filter_response(library_tag_resp),\n",
    "    **filter_response(library_title_resp),\n",
    "}\n",
    "print(f\"Have {len(questions)} question records total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d190b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the questions data\n",
    "\n",
    "from llm_cgr import save_json\n",
    "from datetime import datetime\n",
    "\n",
    "file_path = f\"questions_{datetime.now().date()}.json\"\n",
    "save_json(\n",
    "    data=questions,\n",
    "    file_path=file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65019e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a10e4861",
   "metadata": {},
   "source": [
    "# Process the question titles\n",
    "\n",
    "Normalise the titles, break them down into n-grams, and query embeddings for them.\n",
    "Ready for systematic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed24d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to normalise the question titles\n",
    "\n",
    "\n",
    "def process_text(text: str) -> str:\n",
    "    text = text.lower().strip()\n",
    "    for _replace, _with in [\n",
    "        (\"opensource\", \"open source\"),\n",
    "        (\"light-weight\", \"lightweight\"),\n",
    "    ]:\n",
    "        text = text.replace(_replace, _with)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "titles = [process_text(q[\"title\"]) for q in questions.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ef4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract n-grams from the titles\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "X = vectorizer.fit_transform(titles)\n",
    "ngrams = vectorizer.get_feature_names_out()  # all unique n-grams\n",
    "counts = X.toarray().sum(axis=0)  # frequencies of each n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72943882",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ngrams = [ng for ng in ngrams if len(ng) > 3 and not ng.isdigit()]\n",
    "\n",
    "print(f\"Have {len(filtered_ngrams)} unique n-grams after filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77772ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.vocabulary_  # dict: ngram -> column index\n",
    "ngram_titles = {}\n",
    "for ng in filtered_ngrams:\n",
    "    col = vocab[ng]\n",
    "    # X[:, col] is a sparse column; .nonzero()[0] gives row indices where count>0\n",
    "    rows = X[:, col].nonzero()[0]\n",
    "    ngram_titles[ng] = rows.tolist()\n",
    "\n",
    "print(ngram_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77012807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine n-grams with their counts\n",
    "_combined = zip(ngrams, counts)\n",
    "_combined = sorted(_combined, key=lambda x: x[1], reverse=True)\n",
    "ngram_counts = dict(_combined)\n",
    "\n",
    "# filter out small n-grams and those that are purely numeric\n",
    "ngram_counts = {\n",
    "    k: int(v) for k, v in ngram_counts.items() if len(k) > 2 and not k.isdigit()\n",
    "}\n",
    "\n",
    "print(f\"Have {len(ngram_counts)} n-grams after filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aebf4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the n-grams counts data\n",
    "\n",
    "file_path = f\"ngrams_{datetime.now().date()}.json\"\n",
    "save_json(\n",
    "    data=ngram_titles,\n",
    "    file_path=file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d5d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings for the n-grams\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "_ngrams = list(ngram_counts.keys())\n",
    "embeddings = model.encode(\n",
    "    sentences=_ngrams,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise embeddings (for cosine similarity)\n",
    "\n",
    "embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76288233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine n-grams with their embeddings\n",
    "\n",
    "ngram_embeddings = dict(zip(_ngrams, embeddings))\n",
    "ngram_embeddings = {k: v.tolist() for k, v in ngram_embeddings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c3a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the n-grams embeddings data\n",
    "\n",
    "file_path = f\"embeddings_{datetime.now().date()}.json\"\n",
    "save_json(\n",
    "    data=ngram_embeddings,\n",
    "    file_path=file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1458b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "558718ae",
   "metadata": {},
   "source": [
    "# Get embeddings for the full question titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate out questions and ids\n",
    "\n",
    "question_ids = []\n",
    "question_titles = []\n",
    "\n",
    "for _id, _data in questions.items():\n",
    "    question_ids.append(_id)\n",
    "    question_titles.append(_data[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66600b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to normalise the question titles\n",
    "\n",
    "DENY_LIST = [\n",
    "    # generic terms\n",
    "    \"library\",\n",
    "    \"framework\",\n",
    "    \"libraries\",\n",
    "    \"code\",\n",
    "    \"image\",\n",
    "    \"images\",\n",
    "    \"file\",\n",
    "    \"files\",\n",
    "    \"online\",\n",
    "    \"audio\",\n",
    "    \"graphics\",\n",
    "    \"video\",\n",
    "    \"interactive\",\n",
    "    \"level\",\n",
    "    \"similar\",\n",
    "    \"equivalent\",\n",
    "    \"markup\",\n",
    "    # programming languages\n",
    "    \"python\",\n",
    "    \"javascript\",\n",
    "    \"java\",\n",
    "    \"c++\",\n",
    "    \"c#\",\n",
    "    \"ruby\",\n",
    "    \"php\",\n",
    "    \"c/c++\",\n",
    "    \"js\",\n",
    "    \"go\",\n",
    "    \"rust\",\n",
    "    \"sql\",\n",
    "    \"typescript\",\n",
    "    \"kotlin\",\n",
    "    \"swift\",\n",
    "    \"bash\",\n",
    "    \"shell\",\n",
    "    # file types\n",
    "    \"pdf\",\n",
    "    \"html\",\n",
    "    \"css\",\n",
    "    \"markdown\",\n",
    "    \"json\",\n",
    "    # technologies and platforms\n",
    "    \"android\",\n",
    "    \".net\",\n",
    "    \"windows\",\n",
    "    \"database\",\n",
    "    \"key\",\n",
    "    \"flask\",\n",
    "    \"spring\",\n",
    "    \"laravel\",\n",
    "    \"django\",\n",
    "    \"react\",\n",
    "    \"angular\",\n",
    "    \"vue\",\n",
    "    \"node.js\",\n",
    "]\n",
    "\n",
    "\n",
    "def process_text(text: str) -> str:\n",
    "    text = text.lower().strip()\n",
    "    for _replace, _with in [\n",
    "        (\"opensource\", \"open source\"),\n",
    "        (\"light-weight\", \"lightweight\"),\n",
    "    ]:\n",
    "        text = text.replace(_replace, _with)\n",
    "\n",
    "    for _tech in DENY_LIST:\n",
    "        text = text.replace(_tech, \"<TECH>\")\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "question_titles = [process_text(q) for q in question_titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings for the titles and normalise them\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "title_embeddings = model.encode(\n",
    "    sentences=question_titles,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "title_embeddings = title_embeddings / np.linalg.norm(\n",
    "    title_embeddings, axis=1, keepdims=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d79ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine question ids with their title embeddings\n",
    "\n",
    "title_id_embeddings = dict(zip(question_ids, title_embeddings))\n",
    "title_id_embeddings = {k: v.tolist() for k, v in title_id_embeddings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ba9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the question title embeddings data\n",
    "\n",
    "file_path = f\"question_embeddings_{datetime.now().date()}.json\"\n",
    "save_json(\n",
    "    data=title_id_embeddings,\n",
    "    file_path=file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f045dc2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
