{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7403d950",
   "metadata": {},
   "source": [
    "# Download & Process StackExchange data *(experiment 1 setup)*\n",
    "\n",
    "Query the [Software Recommendations StackExchange](https://softwarerecs.stackexchange.com/) [API](https://api.stackexchange.com/) for questions related to coding libraries.\n",
    "\n",
    "We filter the questions to only those related to libraries, and then use these questions to determine the descriptions to use for our realistic user prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a59e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial set up\n",
    "\n",
    "from llm_cgr import load_json, save_json\n",
    "from datetime import datetime\n",
    "\n",
    "dir = \"../data/stackexchange\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f292bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the client\n",
    "\n",
    "from stackapi import StackAPI\n",
    "\n",
    "site = StackAPI(\"softwarerecs\", max_pages=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38318ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter the response\n",
    "\n",
    "KEYS_TO_SAVE = [\"title\", \"tags\", \"creation_date\", \"link\"]\n",
    "\n",
    "\n",
    "def filter_response(\n",
    "    response: dict,\n",
    ") -> dict:\n",
    "    return {\n",
    "        item[\"question_id\"]: {key: item[key] for key in KEYS_TO_SAVE if key in item}\n",
    "        for item in response[\"items\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1923d",
   "metadata": {},
   "source": [
    "## **1.** Query all recent questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09545088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 2500 records.\n"
     ]
    }
   ],
   "source": [
    "# query maximum questions, sorted by most recent creation date\n",
    "\n",
    "recent_response = site.fetch(\n",
    "    endpoint=\"questions\",\n",
    "    sort=\"creation\",\n",
    "    order=\"desc\",\n",
    ")\n",
    "print(f\"Have {len(recent_response['items'])} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d576d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the recent questions data\n",
    "\n",
    "save_json(\n",
    "    data=filter_response(recent_response),\n",
    "    file_path=f\"{dir}/recent_questions_{datetime.now().date()}.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f369ab",
   "metadata": {},
   "source": [
    "## **2.** Sample 200 of the questions for manual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5fc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['84729', '83509', '87742', '87357', '87064', '85045', '84578', '91852', '84334', '92297', '90696', '83586', '83566', '84444', '86984', '87173', '91479', '93389', '83526', '92009', '86751', '91850', '90674', '87015', '90975', '92287', '87794', '83296', '85224', '90699', '88652', '87793', '85185', '86943', '88603', '84572', '84432', '90170', '84499', '88919', '88698', '93397', '87604', '83731', '91049', '91777', '84905', '90156', '84196', '91920', '87998', '89953', '92155', '86659', '84088', '83765', '87113', '87964', '84210', '87174', '84550', '91010', '89987', '85249', '90071', '88863', '86874', '87634', '84108', '93441', '85369', '91757', '87356', '85260', '91073', '90163', '87690', '91968', '86998', '88434', '83906', '87127', '83591', '88316', '90458', '87659', '84040', '86886', '92055', '88296', '86901', '91430', '90386', '91044', '85071', '87606', '87374', '92010', '91800', '87583', '92242', '90772', '92232', '90423', '89955', '86992', '85037', '91516', '91388', '84398', '83779', '84693', '85170', '85227', '90697', '93340', '83999', '90228', '90189', '93331', '91127', '91710', '87443', '91930', '83347', '84770', '91782', '87632', '88004', '90830', '85214', '91012', '83248', '87590', '91445', '85468', '91500', '84626', '88066', '93440', '90103', '85238', '91805', '91719', '83206', '93363', '88432', '91329', '83444', '84732', '89961', '88192', '87263', '83936', '87286', '84311', '91306', '84086', '91742', '84912', '84933', '91207', '91894', '85291', '87616', '91700', '93413', '90708', '86890', '91802', '86773', '88250', '90416', '90101', '90869', '91594', '90992', '84858', '87389', '87085', '84006', '88627', '83463', '92280', '91936', '87147', '92281', '83299', '84103', '83942', '87124', '84063', '83580', '88514', '91564', '87250', '87795', '91303', '86930', '84984']\n"
     ]
    }
   ],
   "source": [
    "# sample 200 of the recent questions for manual analysis\n",
    "\n",
    "import random\n",
    "\n",
    "recent_questions = load_json(\n",
    "    file_path=f\"{dir}/recent_questions_2025-06-30.json\",\n",
    ")\n",
    "recent_ids = sorted(recent_questions.keys())\n",
    "\n",
    "random.seed(42)  # for reproducibility\n",
    "recent_sample_ids = random.sample(\n",
    "    population=recent_ids,\n",
    "    k=200,\n",
    ")\n",
    "print(recent_sample_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64656fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save random sample with dictionary to hold whether the question is library-related\n",
    "\n",
    "recent_sample = {}\n",
    "for question_id in recent_sample_ids:\n",
    "    recent_sample[question_id] = recent_questions[question_id]\n",
    "    recent_sample[question_id][\"is_library_related\"] = {\n",
    "        \"manual\": None,\n",
    "        \"auto\": None,\n",
    "    }\n",
    "\n",
    "save_json(\n",
    "    data=recent_sample,\n",
    "    file_path=f\"{dir}/manual_sample_{datetime.now().date()}.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ddf4f6",
   "metadata": {},
   "source": [
    "## **3.** Define our filter method\n",
    "\n",
    "Filter to questions that:\n",
    "* Contain the words \"library\" or \"framework\"\n",
    "* Have the tag \"library\" or \"framework\"\n",
    "* Are tagged with one of the top 10 programming languages (TIOBE Index), and not tagged with \"books\" or \"ide\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808dd314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our filter for library questions\n",
    "\n",
    "# fmt: off\n",
    "base_tags = [\"library\", \"framework\"]\n",
    "\n",
    "language_tags = [\n",
    "    # top 10 programming languages (TIOBE Index)\n",
    "    \"python\", \"c++\", \"c\", \"java\", \"c#\",\n",
    "    \"javascript\", \"go\", \"visual-basic\", \"pascal\", \"fortran\",\n",
    "]\n",
    "language_blocks = [\"books\", \"ide\"]\n",
    "\n",
    "words = [\"library\", \"framework\"]\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad249f1",
   "metadata": {},
   "source": [
    "## **4.** Apply the filter to the manual sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af3d4e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_sample_file = f\"{dir}/manual_analysis_2025-06-30.json\"\n",
    "manual_sample = load_json(\n",
    "    file_path=manual_sample_file,\n",
    ")\n",
    "\n",
    "for _id, _data in manual_sample.items():\n",
    "    # check for the words in the title\n",
    "    if any(word in _data[\"title\"].lower() for word in words):\n",
    "        manual_sample[_id][\"is_library_related\"][\"auto\"] = True\n",
    "\n",
    "    # check for any base tags\n",
    "    elif any(tag in _data[\"tags\"] for tag in base_tags):\n",
    "        manual_sample[_id][\"is_library_related\"][\"auto\"] = True\n",
    "\n",
    "    # check for any language tags\n",
    "    elif any(tag in _data[\"tags\"] for tag in language_tags) and not any(\n",
    "        tag in _data[\"tags\"] for tag in language_blocks\n",
    "    ):\n",
    "        manual_sample[_id][\"is_library_related\"][\"auto\"] = True\n",
    "\n",
    "    # otherwise not library related\n",
    "    else:\n",
    "        manual_sample[_id][\"is_library_related\"][\"auto\"] = False\n",
    "\n",
    "save_json(\n",
    "    data=manual_sample,\n",
    "    file_path=manual_sample_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0559f3db",
   "metadata": {},
   "source": [
    "## **5.** Check the accuracy of the filter compared to the manual classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c1c6b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       177\n",
      "           1       0.91      0.91      0.91        23\n",
      "\n",
      "    accuracy                           0.98       200\n",
      "   macro avg       0.95      0.95      0.95       200\n",
      "weighted avg       0.98      0.98      0.98       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "manual_sample = load_json(\n",
    "    file_path=f\"{dir}/manual_analysis_2025-06-30.json\",\n",
    ")\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "for _, _data in manual_sample.items():\n",
    "    y_true.append(int(_data[\"is_library_related\"][\"manual\"]))\n",
    "    y_pred.append(int(_data[\"is_library_related\"][\"auto\"]))\n",
    "\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7cf5cb",
   "metadata": {},
   "source": [
    "## **6.** Query questions for each tag and word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63528932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 1022 records for tag library.\n",
      "Have 200 records for tag framework.\n"
     ]
    }
   ],
   "source": [
    "# fetch questions for each tag and filter them\n",
    "\n",
    "tagged_questions = {}\n",
    "for tag in base_tags:\n",
    "    _response = site.fetch(\n",
    "        endpoint=\"questions\",\n",
    "        sort=\"creation\",\n",
    "        order=\"desc\",\n",
    "        tagged=tag,\n",
    "    )\n",
    "    tagged_questions.update(\n",
    "        filter_response(_response),\n",
    "    )\n",
    "    print(f\"Have {len(_response['items'])} records for tag {tag}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8b4b5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 872 records for tag python.\n",
      "Have 575 records for tag c++.\n",
      "Have 262 records for tag c.\n",
      "Have 705 records for tag java.\n",
      "Have 407 records for tag c#.\n",
      "Have 839 records for tag javascript.\n",
      "Have 38 records for tag go.\n",
      "Have 0 records for tag visual-basic.\n",
      "Have 0 records for tag pascal.\n",
      "Have 13 records for tag fortran.\n",
      "Have 3229 language questions after filtering.\n"
     ]
    }
   ],
   "source": [
    "# fetch questions for each tag and filter them\n",
    "\n",
    "language_questions = {}\n",
    "for tag in language_tags:\n",
    "    _response = site.fetch(\n",
    "        endpoint=\"questions\",\n",
    "        sort=\"creation\",\n",
    "        order=\"desc\",\n",
    "        tagged=tag,\n",
    "    )\n",
    "    language_questions.update(\n",
    "        filter_response(_response),\n",
    "    )\n",
    "    print(f\"Have {len(_response['items'])} records for tag {tag}.\")\n",
    "\n",
    "# filter out language questions that are tagged with the blocks\n",
    "language_questions = {\n",
    "    _id: _data\n",
    "    for _id, _data in language_questions.items()\n",
    "    if not any(tag in _data[\"tags\"] for tag in language_blocks)\n",
    "}\n",
    "print(f\"Have {len(language_questions)} language questions after filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf523c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 1197 records for word library.\n",
      "Have 330 records for word framework.\n"
     ]
    }
   ],
   "source": [
    "# fetch questions for each word and filter them\n",
    "\n",
    "word_questions = {}\n",
    "for word in words:\n",
    "    _response = site.fetch(\n",
    "        endpoint=\"search/advanced\",\n",
    "        sort=\"creation\",\n",
    "        order=\"desc\",\n",
    "        title=word,\n",
    "    )\n",
    "    word_questions.update(\n",
    "        filter_response(_response),\n",
    "    )\n",
    "    print(f\"Have {len(_response['items'])} records for word {word}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5cb2737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 3917 question records total:\n",
      "    (1207 questions from base tags,\n",
      "     3229 questions from language tags,\n",
      "     1499 questions from title words.)\n"
     ]
    }
   ],
   "source": [
    "all_questions = {\n",
    "    **tagged_questions,\n",
    "    **language_questions,\n",
    "    **word_questions,\n",
    "}\n",
    "print(f\"Have {len(all_questions)} question records total:\")\n",
    "\n",
    "print(f\"    ({len(tagged_questions)} questions from base tags,\")\n",
    "print(f\"     {len(language_questions)} questions from language tags,\")\n",
    "print(f\"     {len(word_questions)} questions from title words.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d190b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all the questions data, ready for analysis\n",
    "\n",
    "save_json(\n",
    "    data=all_questions,\n",
    "    file_path=f\"{dir}/library_questions_{datetime.now().date()}.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10e4861",
   "metadata": {},
   "source": [
    "## **7.** Extract n-grams from the question titles\n",
    "\n",
    "Normalise the titles and break them down into n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed24d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to normalise the question titles\n",
    "\n",
    "\n",
    "def process_title(text: str) -> str:\n",
    "    text = text.lower().strip()\n",
    "    for _replace, _with in [\n",
    "        (\"opensource\", \"open source\"),\n",
    "        (\"light-weight\", \"lightweight\"),\n",
    "    ]:\n",
    "        text = text.replace(_replace, _with)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2b77ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the questions data and process titles\n",
    "\n",
    "questions = load_json(\n",
    "    file_path=f\"{dir}/library_questions_2025-07-04.json\",\n",
    ")\n",
    "\n",
    "ids, titles = [], []\n",
    "for _id, _data in questions.items():\n",
    "    ids.append(_id)\n",
    "    titles.append(process_title(text=_data[\"title\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b05ef4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract n-grams from the titles\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "X = vectorizer.fit_transform(titles)\n",
    "ngrams = vectorizer.get_feature_names_out()  # all unique n-grams\n",
    "vocab = vectorizer.vocabulary_  # dict: ngram -> column index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72943882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 35641 unique n-grams after filtering.\n"
     ]
    }
   ],
   "source": [
    "# filter out n-grams that are too short or are numeric\n",
    "\n",
    "filtered_ngrams = [ng for ng in ngrams if len(ng) > 2 and not ng.isdigit()]\n",
    "\n",
    "print(f\"Have {len(filtered_ngrams)} unique n-grams after filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77772ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 35641 n-grams mapped to their titles.\n"
     ]
    }
   ],
   "source": [
    "# map ngrams to title ids\n",
    "\n",
    "ngram_titles = {}\n",
    "for ng in filtered_ngrams:\n",
    "    col = vocab[ng]\n",
    "    rows = X[:, col].nonzero()[0]\n",
    "    ngram_titles[ng] = [ids[i] for i in rows.tolist()]\n",
    "\n",
    "print(f\"Have {len(ngram_titles)} n-grams mapped to their titles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aebf4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the n-grams data\n",
    "\n",
    "save_json(\n",
    "    data=ngram_titles,\n",
    "    file_path=f\"{dir}/ngrams_{datetime.now().date()}.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1458b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
