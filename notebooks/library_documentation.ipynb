{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d384ca",
   "metadata": {},
   "source": [
    "# Download all the library members from documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60ed3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need documentation for 30 libraries: ['bs4', 'chardet', 'cryptography', 'dateutil', 'django', 'folium', 'librosa', 'lxml', 'matplotlib', 'nltk', 'numpy', 'openpyxl', 'pandas', 'psutil', 'pytesseract', 'pytz', 'regex', 'requests', 'scipy', 'seaborn', 'sklearn', 'statsmodels', 'sympy', 'tensorflow', 'textblob', 'texttable', 'wordcloud', 'wordninja', 'xlwt', 'xmltodict']\n"
     ]
    }
   ],
   "source": [
    "# get list of libraries we need for the study\n",
    "\n",
    "from src.constants import DOCUMENTED_LIBRARIES\n",
    "\n",
    "print(\n",
    "    f\"Need documentation for {len(DOCUMENTED_LIBRARIES)} libraries: {DOCUMENTED_LIBRARIES}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00efa1ae",
   "metadata": {},
   "source": [
    "## **1.** Manually download 6 smaller libraries without parsable documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "893e8ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 6 manually scraped libraries.\n"
     ]
    }
   ],
   "source": [
    "# small libraries, scraped manually from the source code\n",
    "\n",
    "manually_scraped = {\n",
    "    \"wordninja\": {\n",
    "        \"url\": \"https://github.com/keredson/wordninja/blob/master/wordninja.py\",\n",
    "        \"version\": \"2.0.0\",\n",
    "        \"members\": [\n",
    "            \"wordninja\",\n",
    "            \"wordninja.LanguageModel\",\n",
    "            \"wordninja.DEFAULT_LANGUAGE_MODEL\",\n",
    "            \"wordninja.split\",\n",
    "        ],\n",
    "    },\n",
    "    \"texttable\": {\n",
    "        \"url\": \"https://github.com/foutaise/texttable/blob/master/texttable.py\",\n",
    "        \"version\": \"1.7.0\",\n",
    "        \"members\": [\n",
    "            \"texttable\",\n",
    "            \"texttable.Texttable\",\n",
    "            \"texttable.ArraySizeError\",\n",
    "            \"texttable.obj2unicode\",\n",
    "            \"texttable.len\",\n",
    "        ],\n",
    "    },\n",
    "    \"xmltodict\": {\n",
    "        \"url\": \"https://github.com/martinblech/xmltodict/blob/master/xmltodict.py\",\n",
    "        \"version\": \"0.14.2\",\n",
    "        \"members\": [\n",
    "            \"xmltodict\",\n",
    "            \"xmltodict.parse\",\n",
    "            \"xmltodict.unparse\",\n",
    "            \"xmltodict.ParsingInterrupted\",\n",
    "        ],\n",
    "    },\n",
    "    \"regex\": {\n",
    "        \"url\": \"https://github.com/mrabarnett/mrab-regex/blob/hg/regex_3/regex.py\",\n",
    "        \"version\": \"2.5.153\",\n",
    "        \"members\": [\n",
    "            \"regex\",\n",
    "            \"regex.cache_all\",\n",
    "            \"regex.compile\",\n",
    "            \"regex.DEFAULT_VERSION\",\n",
    "            \"regex.escape\",\n",
    "            \"regex.findall\",\n",
    "            \"regex.finditer\",\n",
    "            \"regex.fullmatch\",\n",
    "            \"regex.match\",\n",
    "            \"regex.purge\",\n",
    "            \"regex.search\",\n",
    "            \"regex.split\",\n",
    "            \"regex.splititer\",\n",
    "            \"regex.sub\",\n",
    "            \"regex.subf\",\n",
    "            \"regex.subfn\",\n",
    "            \"regex.subn\",\n",
    "            \"regex.template\",\n",
    "            \"regex.Scanner\",\n",
    "            \"regex.A\",\n",
    "            \"regex.ASCII\",\n",
    "            \"regex.B\",\n",
    "            \"regex.BESTMATCH\",\n",
    "            \"regex.D\",\n",
    "            \"regex.DEBUG\",\n",
    "            \"regex.E\",\n",
    "            \"regex.ENHANCEMATCH\",\n",
    "            \"regex.S\",\n",
    "            \"regex.DOTALL\",\n",
    "            \"regex.F\",\n",
    "            \"regex.FULLCASE\",\n",
    "            \"regex.I\",\n",
    "            \"regex.IGNORECASE\",\n",
    "            \"regex.L\",\n",
    "            \"regex.LOCALE\",\n",
    "            \"regex.M\",\n",
    "            \"regex.MULTILINE\",\n",
    "            \"regex.P\",\n",
    "            \"regex.POSIX\",\n",
    "            \"regex.R\",\n",
    "            \"regex.REVERSE\",\n",
    "            \"regex.T\",\n",
    "            \"regex.TEMPLATE\",\n",
    "            \"regex.U\",\n",
    "            \"regex.UNICODE\",\n",
    "            \"regex.V0\",\n",
    "            \"regex.VERSION0\",\n",
    "            \"regex.V1\",\n",
    "            \"regex.VERSION1\",\n",
    "            \"regex.X\",\n",
    "            \"regex.VERBOSE\",\n",
    "            \"regex.W\",\n",
    "            \"regex.WORD\",\n",
    "            \"regex.error\",\n",
    "            \"regex.Regex\",\n",
    "            \"regex.__version__\",\n",
    "            \"regex.__doc__\",\n",
    "            \"regex.RegexFlag\",\n",
    "        ],\n",
    "    },\n",
    "    \"pytz\": {\n",
    "        \"url\": \"https://github.com/stub42/pytz/blob/master/src/pytz/__init__.py\",\n",
    "        \"version\": \"2025.2\",\n",
    "        \"members\": [\n",
    "            \"pytz\",\n",
    "            \"pytz.timezone\",\n",
    "            \"pytz.utc\",\n",
    "            \"pytz.country_timezones\",\n",
    "            \"pytz.country_names\",\n",
    "            \"pytz.all_timezones\",\n",
    "            \"pytz.all_timezones_set\",\n",
    "            \"pytz.common_timezones\",\n",
    "            \"pytz.common_timezones_set\",\n",
    "            \"pytz.BaseTzInfo\",\n",
    "            \"pytz.FixedOffset\",\n",
    "            \"pytz.AmbiguousTimeError\",\n",
    "            \"pytz.InvalidTimeError\",\n",
    "            \"pytz.NonExistentTimeError\",\n",
    "            \"pytz.UnknownTimeZoneError\",\n",
    "            \"pytz.exceptions\",\n",
    "            \"pytz.exceptions.AmbiguousTimeError\",\n",
    "            \"pytz.exceptions.InvalidTimeError\",\n",
    "            \"pytz.exceptions.NonExistentTimeError\",\n",
    "            \"pytz.exceptions.UnknownTimeZoneError\",\n",
    "            \"pytz.tzinfo\",\n",
    "            \"pytz.tzinfo.memorized_timedelta\",\n",
    "            \"pytz.tzinfo.memorized_datetime\",\n",
    "            \"pytz.tzinfo.memorized_ttinfo\",\n",
    "            \"pytz.tzinfo.BaseTzInfo\",\n",
    "            \"pytz.tzinfo.StaticTzInfo\",\n",
    "            \"pytz.tzinfo.DstTzInfo\",\n",
    "            \"pytz.tzinfo.unpickler\",\n",
    "            \"pytz.tzfile\",\n",
    "            \"pytz.tzfile.build_tzinfo\",\n",
    "            \"pytz.reference\",\n",
    "            \"pytz.reference.FixedOffset\",\n",
    "            \"pytz.reference.LocalTimezone\",\n",
    "            \"pytz.reference.USTimeZone\",\n",
    "            \"pytz.reference.Eastern\",\n",
    "            \"pytz.reference.Central\",\n",
    "            \"pytz.reference.Mountain\",\n",
    "            \"pytz.reference.Pacific\",\n",
    "            \"pytz.reference.UTC\",\n",
    "            \"pytz.lazy\",\n",
    "            \"pytz.lazy.LazyDict\",\n",
    "            \"pytz.lazy.LazyList\",\n",
    "            \"pytz.lazy.LazySet\",\n",
    "        ],\n",
    "    },\n",
    "    \"pytesseract\": {\n",
    "        \"url\": \"https://github.com/madmaze/pytesseract\",\n",
    "        \"version\": \"0.3.13\",\n",
    "        \"members\": [\n",
    "            \"pytesseract\",\n",
    "            \"pytesseract.DEFAULT_ENCODING\",\n",
    "            \"pytesseract.LANG_PATTERN\",\n",
    "            \"pytesseract.RGB_MODE\",\n",
    "            \"pytesseract.SUPPORTED_FORMATS\",\n",
    "            \"pytesseract.OSD_KEYS\",\n",
    "            \"pytesseract.EXTENTION_TO_CONFIG\",\n",
    "            \"pytesseract.TESSERACT_MIN_VERSION\",\n",
    "            \"pytesseract.TESSERACT_ALTO_VERSION\",\n",
    "            \"pytesseract.Output\",\n",
    "            \"pytesseract.PandasNotSupported\",\n",
    "            \"pytesseract.TesseractError\",\n",
    "            \"pytesseract.TesseractNotFoundError\",\n",
    "            \"pytesseract.TSVNotSupported\",\n",
    "            \"pytesseract.ALTONotSupported\",\n",
    "            \"pytesseract.kill\",\n",
    "            \"pytesseract.timeout_manager\",\n",
    "            \"pytesseract.run_once\",\n",
    "            \"pytesseract.get_errors\",\n",
    "            \"pytesseract.cleanup\",\n",
    "            \"pytesseract.prepare\",\n",
    "            \"pytesseract.save\",\n",
    "            \"pytesseract.subprocess_args\",\n",
    "            \"pytesseract.run_tesseract\",\n",
    "            \"pytesseract.run_and_get_multiple_output\",\n",
    "            \"pytesseract.run_and_get_output\",\n",
    "            \"pytesseract.file_to_dict\",\n",
    "            \"pytesseract.is_valid\",\n",
    "            \"pytesseract.osd_to_dict\",\n",
    "            \"pytesseract.get_languages\",\n",
    "            \"pytesseract.get_tesseract_version\",\n",
    "            \"pytesseract.image_to_string\",\n",
    "            \"pytesseract.image_to_pdf_or_hocr\",\n",
    "            \"pytesseract.image_to_alto_xml\",\n",
    "            \"pytesseract.image_to_boxes\",\n",
    "            \"pytesseract.get_pandas_output\",\n",
    "            \"pytesseract.image_to_data\",\n",
    "            \"pytesseract.image_to_osd\",\n",
    "            \"pytesseract.main\",\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"Have {len(manually_scraped)} manually scraped libraries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c260e",
   "metadata": {},
   "source": [
    "## **2.** Automatically download the inventory of 21 libraries using Sphinx / readthedocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aca33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the inventory urls for libraries with sphinx / readthedocs documentation\n",
    "\n",
    "inventory_urls = {\n",
    "    \"openpyxl\": \"https://openpyxl.readthedocs.io/en/latest/objects.inv\",\n",
    "    \"django\": \"https://docs.djangoproject.com/en/stable/objects.inv\",\n",
    "    \"statsmodels\": \"https://www.statsmodels.org/stable/objects.inv\",\n",
    "    \"wordcloud\": \"https://amueller.github.io/word_cloud/objects.inv\",\n",
    "    \"librosa\": \"https://librosa.org/doc/latest/objects.inv\",\n",
    "    \"psutil\": \"https://psutil.readthedocs.io/en/latest/objects.inv\",\n",
    "    \"chardet\": \"https://chardet.readthedocs.io/en/latest/objects.inv\",\n",
    "    \"textblob\": \"https://textblob.readthedocs.io/en/latest/objects.inv\",\n",
    "    \"xlwt\": \"https://xlwt.readthedocs.io/en/latest/objects.inv\",\n",
    "    \"dateutil\": \"https://dateutil.readthedocs.io/en/stable/objects.inv\",\n",
    "    \"scipy\": \"https://docs.scipy.org/doc/scipy/objects.inv\",\n",
    "    \"seaborn\": \"https://seaborn.pydata.org/objects.inv\",\n",
    "    \"cryptography\": \"https://cryptography.io/en/latest/objects.inv\",\n",
    "    \"pandas\": \"https://pandas.pydata.org/pandas-docs/stable/objects.inv\",\n",
    "    \"numpy\": \"https://numpy.org/doc/stable/objects.inv\",\n",
    "    \"sklearn\": \"https://scikit-learn.org/stable/objects.inv\",\n",
    "    \"matplotlib\": \"https://matplotlib.org/stable/objects.inv\",\n",
    "    \"sympy\": \"https://docs.sympy.org/latest/objects.inv\",\n",
    "    \"requests\": \"https://docs.python-requests.org/en/latest/objects.inv\",\n",
    "    \"bs4\": \"https://www.crummy.com/software/BeautifulSoup/bs4/doc/objects.inv\",\n",
    "    \"folium\": \"https://python-visualization.github.io/folium/latest/objects.inv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a51f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define method to fetch the inventory and extract importable python objects\n",
    "\n",
    "import sphobjinv as soi\n",
    "\n",
    "python_objects = {\n",
    "    \"module\",\n",
    "    \"class\",\n",
    "    \"exception\",\n",
    "    \"function\",\n",
    "    \"data\",\n",
    "    # all of these are parts of classes, and not importable on their own, so ignored\n",
    "    # \"method\", \"attribute\", \"property\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_library_objects(library: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch the inventory from the given URL and return a dictionary with library info.\n",
    "    \"\"\"\n",
    "    # download the inventory\n",
    "    print(f\"Scraping {library}...\")\n",
    "    inv = soi.Inventory(url=inventory_urls[library])\n",
    "\n",
    "    # filter to only importable python objects\n",
    "    members = set()\n",
    "    for obj in inv.objects:\n",
    "        if obj.domain == \"py\" and obj.role in python_objects:\n",
    "            members.add(obj.name)\n",
    "\n",
    "    print(f\"Have {len(members)} members for {library}.\")\n",
    "    return {\n",
    "        \"url\": inventory_urls[library],\n",
    "        \"version\": inv.version,\n",
    "        \"toplevels\": sorted(set(m.split(\".\")[0] for m in members)),\n",
    "        \"members\": sorted(members),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80876336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping openpyxl...\n",
      "Have 799 members for openpyxl.\n",
      "Scraping django...\n",
      "Have 1163 members for django.\n",
      "Scraping statsmodels...\n",
      "Have 935 members for statsmodels.\n",
      "Scraping wordcloud...\n",
      "Have 5 members for wordcloud.\n",
      "Scraping librosa...\n",
      "Have 232 members for librosa.\n",
      "Scraping psutil...\n",
      "Have 118 members for psutil.\n",
      "Scraping chardet...\n",
      "Have 68 members for chardet.\n",
      "Scraping textblob...\n",
      "Have 55 members for textblob.\n",
      "Scraping xlwt...\n",
      "Have 15 members for xlwt.\n",
      "Scraping dateutil...\n",
      "Have 50 members for dateutil.\n",
      "Scraping scipy...\n",
      "Have 2626 members for scipy.\n",
      "Scraping seaborn...\n",
      "Have 133 members for seaborn.\n",
      "Scraping cryptography...\n",
      "Have 454 members for cryptography.\n",
      "Scraping pandas...\n",
      "Have 356 members for pandas.\n",
      "Scraping numpy...\n",
      "Have 1268 members for numpy.\n",
      "Scraping sklearn...\n",
      "Have 851 members for sklearn.\n",
      "Scraping matplotlib...\n",
      "Have 1188 members for matplotlib.\n",
      "Scraping sympy...\n",
      "Have 3030 members for sympy.\n",
      "Scraping requests...\n",
      "Have 48 members for requests.\n",
      "Scraping bs4...\n",
      "Have 133 members for bs4.\n",
      "Scraping folium...\n",
      "Have 129 members for folium.\n"
     ]
    }
   ],
   "source": [
    "# download the members of the libraries\n",
    "\n",
    "sphinx_scraped = {}\n",
    "for library in inventory_urls:\n",
    "    try:\n",
    "        info = get_library_objects(library)\n",
    "        sphinx_scraped[library] = info\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {library}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ce025",
   "metadata": {},
   "source": [
    "## **3.** Scrape members for the 3 remaining libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbba2ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining = {\n",
    "    \"nltk\": {\n",
    "        \"url\": \"https://www.nltk.org/py-modindex.html\",\n",
    "        \"version\": \"3.8.1\",\n",
    "    },\n",
    "    \"tensorflow\": {\n",
    "        \"url\": \"https://www.tensorflow.org/api_docs/python/tf\",\n",
    "        \"version\": \"2.6.0\",\n",
    "    },\n",
    "    \"lxml\": {\n",
    "        \"urls\": [\n",
    "            \"https://lxml.de/apidoc/index.html\",\n",
    "            \"https://lxml.de/apidoc/lxml.html.html\",\n",
    "            \"https://lxml.de/apidoc/lxml.isoschematron.html\",\n",
    "        ],\n",
    "        \"version\": \"4.6.3\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5de245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to access the html content of documentation pages\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_html_soup(url: str) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Fetch the HTML content from the given URL and return a BeautifulSoup object.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an error for bad responses\n",
    "    return BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36cf0944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 355 members for nltk:\n",
      "\t['nltk', 'nltk.app', 'nltk.app.chartparser_app', 'nltk.app.chunkparser_app', 'nltk.app.collocations_app']\n"
     ]
    }
   ],
   "source": [
    "nltk_soup = get_html_soup(remaining[\"nltk\"][\"url\"])\n",
    "\n",
    "nltk_members = {\"nltk\"}\n",
    "# extract all links from the nltk module index page\n",
    "for a in nltk_soup.find_all(\"a\", href=True):\n",
    "    name = a.text.strip()\n",
    "\n",
    "    # filter entries that look like nltk module paths\n",
    "    if name.startswith(\"nltk.\"):\n",
    "        nltk_members.add(name)\n",
    "\n",
    "nltk_members = sorted(nltk_members)\n",
    "remaining[\"nltk\"][\"toplevels\"] = sorted(set(m.split(\".\")[0] for m in nltk_members))\n",
    "remaining[\"nltk\"][\"members\"] = nltk_members\n",
    "print(f\"Have {len(nltk_members)} members for nltk:\\n\\t{nltk_members[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719a2b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 4377 members for tensorflow:\n",
      "\t['tensorflow.AggregationMethod', 'tensorflow.CriticalSection', 'tensorflow.DeviceSpec', 'tensorflow.GradientTape', 'tensorflow.Graph', 'tensorflow.IndexedSlices', 'tensorflow.IndexedSlicesSpec', 'tensorflow.Module', 'tensorflow.Operation', 'tensorflow.OptionalSpec']\n"
     ]
    }
   ],
   "source": [
    "tf_soup = get_html_soup(remaining[\"tensorflow\"][\"url\"])\n",
    "\n",
    "tf_members = set()\n",
    "# extract links from expandable navigation bar\n",
    "for div in tf_soup.select(\"li.devsite-nav-expandable\"):\n",
    "    # get top level module name from the toggle title span\n",
    "    reference = div.select_one(\"span.devsite-nav-text\")\n",
    "    if not reference or not reference.text.startswith(\"tf\"):\n",
    "        continue\n",
    "\n",
    "    # clean up the module name\n",
    "    title_text = reference.text.strip().replace(\"\\u200b\", \"\").replace(\" \", \"\")\n",
    "    tf_module = title_text.replace(\"tf\", \"tensorflow\")\n",
    "\n",
    "    # within the expandable div, find all links to members\n",
    "    for a in div.select(\"li.devsite-nav-item a.devsite-nav-title\"):\n",
    "        member_text = a.select_one(\"span.devsite-nav-text\").get_text().strip()\n",
    "        member_text = member_text.replace(\"\\u200b\", \"\").replace(\" \", \"\")\n",
    "\n",
    "        # skip overview entries\n",
    "        if member_text.lower() == \"overview\":\n",
    "            continue\n",
    "\n",
    "        # add full module name\n",
    "        tf_members.add(f\"{tf_module}.{member_text}\")\n",
    "\n",
    "tf_members = sorted(tf_members)\n",
    "remaining[\"tensorflow\"][\"toplevels\"] = sorted(set(m.split(\".\")[0] for m in tf_members))\n",
    "remaining[\"tensorflow\"][\"members\"] = tf_members\n",
    "print(f\"Have {len(tf_members)} members for tensorflow:\\n\\t{tf_members[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29e1abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define method to scrape lxml documentation page\n",
    "def get_lxml_members(url: str, package: str, top_level: int) -> set:\n",
    "    _lxml_soup = get_html_soup(url=url)\n",
    "    _lxml_members = set()\n",
    "\n",
    "    # extract all top level modules\n",
    "    for top in _lxml_soup.select(f\"li.toctree-l{top_level}\"):\n",
    "        reference = top.select_one(\"a.reference\")\n",
    "        module = reference.text.strip() if reference else \"\"\n",
    "        module = module.split(\" \")[0]\n",
    "        if (\n",
    "            not module\n",
    "            or not module.startswith(f\"{package}.\")\n",
    "            or module.startswith(f\"{package}._\")\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        _lxml_members.add(module)\n",
    "\n",
    "        # extract all members of the top level modules\n",
    "        for a in top.select(f\"li.toctree-l{top_level + 1} > a\"):\n",
    "            name = a.text.strip()\n",
    "            # Some names include trailing ' — description'; strip that\n",
    "            if name.startswith(\"_\") or name.lower() in [\n",
    "                \"submodules\",\n",
    "                \"module contents\",\n",
    "            ]:\n",
    "                continue\n",
    "\n",
    "            name = name.split(\"—\", 1)[0].strip().rstrip(\"()\")\n",
    "            _lxml_members.add(f\"{module}.{name}\")\n",
    "\n",
    "    # extract all members of the module itself\n",
    "    for dt in _lxml_soup.select(\"dt.sig.sig-object.py\"):\n",
    "        if (_id := dt.get(\"id\")) and dt.select_one(\"span.descclassname\"):\n",
    "            if _id.startswith(f\"{package}.\") and not _id.startswith(f\"{package}._\"):\n",
    "                _lxml_members.add(_id)\n",
    "\n",
    "    return _lxml_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e12a9ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lxml.etree.DocInfo', 'lxml.etree.XSLTExtensionError', 'lxml.etree.XMLDTDID', 'lxml.etree.strip_attributes', 'lxml.sax.SaxError', 'lxml.objectify.ObjectifyElementClassLookup', 'lxml.etree.ETXPath', 'lxml.etree.LxmlError', 'lxml.etree.AncestorsIterator', 'lxml.doctestcompare.strip', 'lxml.etree.PI', 'lxml.etree.NamespaceRegistryError', 'lxml.etree.strip_tags', 'lxml.etree.Element', 'lxml.etree.iterparse', 'lxml.html.ElementSoup', 'lxml.doctestcompare.LXMLOutputChecker', 'lxml.etree.LxmlRegistryError', 'lxml.etree.XSLTExtension', 'lxml.html.defs', 'lxml.etree.dump', 'lxml.etree.XPathEvaluator', 'lxml.etree.XPathEvalError', 'lxml.objectify.annotate', 'lxml.etree.ParserBasedElementClassLookup', 'lxml.builder.ElementMaker', 'lxml.etree.SiblingsIterator', 'lxml.etree.ETCompatXMLParser', 'lxml.etree.XMLID', 'lxml.etree.ElementDefaultClassLookup', 'lxml.objectify.ObjectPath', 'lxml.objectify.ObjectifiedElement', 'lxml.objectify.NoneElement', 'lxml.etree.ElementNamespaceClassLookup', 'lxml.etree.XMLPullParser', 'lxml.ElementInclude.include', 'lxml.etree.ErrorTypes', 'lxml.etree.XMLParser', 'lxml.etree.iterwalk', 'lxml.etree.register_namespace', 'lxml.etree.ParserError', 'lxml.objectify.IntElement', 'lxml.etree.tostringlist', 'lxml.doctestcompare.LHTMLOutputChecker', 'lxml.etree.htmlfile', 'lxml.etree.SchematronParseError', 'lxml.etree.XPathSyntaxError', 'lxml.objectify.FloatElement', 'lxml.etree.CustomElementClassLookup', 'lxml.objectify.StringElement', 'lxml.objectify.enable_recursive_str', 'lxml.etree.ElementDepthFirstIterator', 'lxml.sax', 'lxml.objectify.XML', 'lxml.doctestcompare.norm_whitespace', 'lxml.objectify.NumberElement', 'lxml.etree.adopt_external_document', 'lxml.etree.DocumentInvalid', 'lxml.html', 'lxml.etree.Error', 'lxml.objectify.PyType', 'lxml.etree.xmlfile', 'lxml.etree.SerialisationError', 'lxml.etree.SchematronError', 'lxml.etree.QName', 'lxml.etree.XMLTreeBuilder', 'lxml.objectify.deannotate', 'lxml.sax.ElementTreeProducer', 'lxml.isoschematron', 'lxml.etree.XSLTSaveError', 'lxml.objectify.set_default_parser', 'lxml.objectify.xsiannotate', 'lxml.etree.HTMLPullParser', 'lxml.html.diff', 'lxml.objectify.getRegisteredTypes', 'lxml.etree.C14NWriterTarget', 'lxml.html.builder', 'lxml.etree.clear_error_log', 'lxml.etree.LxmlSyntaxError', 'lxml.ElementInclude.FatalIncludeError', 'lxml.etree.XInclude', 'lxml.etree.XSLTAccessControl', 'lxml.objectify.ElementMaker', 'lxml.etree.XPathError', 'lxml.objectify.set_pytype_attribute_tag', 'lxml.etree.DTD', 'lxml.etree.canonicalize', 'lxml.etree.RelaxNGParseError', 'lxml.objectify.Element', 'lxml.etree.PyErrorLog', 'lxml.etree.PythonElementClassLookup', 'lxml.etree.set_default_parser', 'lxml.etree.RelaxNGValidateError', 'lxml.etree.XMLSchemaValidateError', 'lxml.ElementInclude', 'lxml.etree.strip_elements', 'lxml.etree.XSLTApplyError', 'lxml.etree.RelaxNGErrorTypes', 'lxml.etree.CommentBase', 'lxml.html.formfill', 'lxml.etree.parseid', 'lxml.sax.saxify', 'lxml.doctestcompare.install', 'lxml.etree.XMLSyntaxAssertionError', 'lxml.etree.DTDError', 'lxml.etree.ErrorDomains', 'lxml.etree.XPath', 'lxml.html.html5parser', 'lxml.etree.ElementTextIterator', 'lxml.doctestcompare.html_fromstring', 'lxml.etree.parse', 'lxml.objectify.fromstring', 'lxml.etree.ErrorLevels', 'lxml.etree.RelaxNGError', 'lxml.html.soupparser', 'lxml.doctestcompare.temp_install', 'lxml.etree.XSLTError', 'lxml.etree.HTMLParser', 'lxml.etree.XSLTParseError', 'lxml.etree.use_global_python_log', 'lxml.etree.CDATA', 'lxml.objectify.DataElement', 'lxml.etree.AttributeBasedElementClassLookup', 'lxml.sax.ElementTreeContentHandler', 'lxml.ElementInclude.LimitedRecursiveIncludeError', 'lxml.etree.XPathFunctionError', 'lxml.ElementInclude.default_loader', 'lxml.etree.XPathDocumentEvaluator', 'lxml.etree.Entity', 'lxml.doctestcompare', 'lxml.etree.SubElement', 'lxml.etree.EntityBase', 'lxml.objectify.parse', 'lxml.html.clean', 'lxml.etree.HTML', 'lxml.etree.XML', 'lxml.etree.cleanup_namespaces', 'lxml.etree.ElementChildIterator', 'lxml.etree.Schematron', 'lxml.etree.XMLSchemaError', 'lxml.etree.XMLSyntaxError', 'lxml.etree.XMLSchema', 'lxml.etree.DTDParseError', 'lxml.etree.C14NError', 'lxml.etree.FallbackElementClassLookup', 'lxml.etree.XSLT', 'lxml.etree.ProcessingInstruction', 'lxml.etree.XPathResultError', 'lxml.objectify.ObjectifiedDataElement', 'lxml.cssselect', 'lxml.etree.indent', 'lxml.etree.set_element_class_lookup', 'lxml.etree.fromstring', 'lxml.objectify.makeparser', 'lxml.etree.fromstringlist', 'lxml.etree.SchematronValidateError', 'lxml.objectify.BoolElement', 'lxml.etree.FunctionNamespace', 'lxml.etree.Resolver', 'lxml.etree.get_default_parser', 'lxml.etree.Extension', 'lxml.etree.ElementTree', 'lxml.etree.iselement', 'lxml.etree.DTDValidateError', 'lxml.etree.TreeBuilder', 'lxml.etree.XIncludeError', 'lxml.etree.XPathElementEvaluator', 'lxml.etree.ParseError', 'lxml.etree', 'lxml.objectify.dump', 'lxml.objectify.pyannotate', 'lxml.etree.ElementClassLookup', 'lxml.etree.ElementBase', 'lxml.etree.Comment', 'lxml.etree.tounicode', 'lxml.objectify.pytypename', 'lxml.builder', 'lxml.etree.tostring', 'lxml.etree.PIBase', 'lxml.objectify', 'lxml.etree.XMLSchemaParseError', 'lxml.etree.RelaxNG'}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    get_lxml_members(\n",
    "        url=remaining[\"lxml\"][\"urls\"][0],\n",
    "        package=\"lxml\",\n",
    "        top_level=3,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db9cb31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 391 members for lxml:\n",
      "\t['lxml', 'lxml.ElementInclude', 'lxml.ElementInclude.FatalIncludeError', 'lxml.ElementInclude.LimitedRecursiveIncludeError', 'lxml.ElementInclude.default_loader', 'lxml.ElementInclude.include', 'lxml.builder', 'lxml.builder.ElementMaker', 'lxml.cssselect', 'lxml.doctestcompare']\n"
     ]
    }
   ],
   "source": [
    "# some manually scraped members\n",
    "lxml_members = {\"lxml\", \"lxml.get_include\"}\n",
    "\n",
    "lxml_members.update(\n",
    "    get_lxml_members(\n",
    "        url=remaining[\"lxml\"][\"urls\"][0],\n",
    "        package=\"lxml\",\n",
    "        top_level=3,\n",
    "    )\n",
    ")\n",
    "lxml_members.update(\n",
    "    get_lxml_members(\n",
    "        url=remaining[\"lxml\"][\"urls\"][1],\n",
    "        package=\"lxml.html\",\n",
    "        top_level=1,\n",
    "    )\n",
    ")\n",
    "lxml_members.update(\n",
    "    get_lxml_members(\n",
    "        url=remaining[\"lxml\"][\"urls\"][2],\n",
    "        package=\"lxml.isoschematron\",\n",
    "        top_level=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "lxml_members = sorted(lxml_members)\n",
    "remaining[\"lxml\"][\"toplevels\"] = sorted(set(m.split(\".\")[0] for m in lxml_members))\n",
    "remaining[\"lxml\"][\"members\"] = lxml_members\n",
    "print(f\"Have {len(lxml_members)} members for lxml:\\n\\t{lxml_members[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb2710",
   "metadata": {},
   "source": [
    "## **4.** Save all of the documentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2070eb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have data for all 30 libraries:\n",
      "\t['bs4', 'chardet', 'cryptography', 'dateutil', 'django', 'folium', 'librosa', 'lxml', 'matplotlib', 'nltk', 'numpy', 'openpyxl', 'pandas', 'psutil', 'pytesseract', 'pytz', 'regex', 'requests', 'scipy', 'seaborn', 'sklearn', 'statsmodels', 'sympy', 'tensorflow', 'textblob', 'texttable', 'wordcloud', 'wordninja', 'xlwt', 'xmltodict']\n"
     ]
    }
   ],
   "source": [
    "from llm_cgr import save_json\n",
    "\n",
    "final_data = {\n",
    "    **manually_scraped,\n",
    "    **sphinx_scraped,\n",
    "    **remaining,\n",
    "}\n",
    "\n",
    "for library in DOCUMENTED_LIBRARIES:\n",
    "    assert library in final_data, f\"Missing documentation for {library}!\"\n",
    "\n",
    "print(\n",
    "    f\"Have data for all {len(DOCUMENTED_LIBRARIES)} libraries:\\n\\t{DOCUMENTED_LIBRARIES}\"\n",
    ")\n",
    "save_json(final_data, \"../data/libraries/members.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
